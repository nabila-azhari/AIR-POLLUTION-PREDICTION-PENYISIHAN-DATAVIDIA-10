{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc3927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FINAL FIXED SUBMISSION GENERATED â†’ submission_FIXED.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FINAL FIXED ISPU PIPELINE â€” REAL FORECAST POLUTAN â†’ MAX\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ===============================\n",
    "# LOAD\n",
    "# ===============================\n",
    "df = pd.read_csv(\"merged_libur_cuaca_ispu_ndvi.csv\", sep=\";\")\n",
    "df[\"tanggal\"] = pd.to_datetime(df[\"tanggal\"], dayfirst=True)\n",
    "df = df.sort_values([\"lokasi_clean\",\"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "POLLUTANTS = [\"pm_sepuluh\",\"pm_duakomalima\",\"ozon\"]\n",
    "\n",
    "# ===============================\n",
    "# WEATHER FEATURES (PENTING!)\n",
    "# ===============================\n",
    "WEATHER = [\n",
    "    c for c in df.columns\n",
    "    if any(k in c.lower() for k in\n",
    "           [\"hujan\",\"angin\",\"suhu\",\"lembab\",\"pressure\",\"radiasi\",\"ndvi\"])\n",
    "]\n",
    "\n",
    "df = df.dropna(subset=POLLUTANTS)\n",
    "\n",
    "# ===============================\n",
    "# REINDEX\n",
    "# ===============================\n",
    "def reindex_daily(g):\n",
    "    idx = pd.date_range(g[\"tanggal\"].min(), g[\"tanggal\"].max(), freq=\"D\")\n",
    "    g = g.set_index(\"tanggal\").reindex(idx)\n",
    "    g[\"lokasi_clean\"] = g[\"lokasi_clean\"].iloc[0]\n",
    "    return g.reset_index().rename(columns={\"index\":\"tanggal\"})\n",
    "\n",
    "df = df.groupby(\"lokasi_clean\", group_keys=False).apply(reindex_daily)\n",
    "df = df.sort_values([\"lokasi_clean\",\"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "# ===============================\n",
    "# TIME FEATURES\n",
    "# ===============================\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"tanggal\"].dt.month/12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"tanggal\"].dt.month/12)\n",
    "df[\"doy_sin\"]   = np.sin(2*np.pi*df[\"tanggal\"].dt.dayofyear/365)\n",
    "df[\"doy_cos\"]   = np.cos(2*np.pi*df[\"tanggal\"].dt.dayofyear/365)\n",
    "\n",
    "TIME_FEATS = [\"month_sin\",\"month_cos\",\"doy_sin\",\"doy_cos\"]\n",
    "\n",
    "# ===============================\n",
    "# LAG BUILDER\n",
    "# ===============================\n",
    "def build_lag(g, col):\n",
    "    g = g.copy()\n",
    "    for l in [1,2,3,7]:\n",
    "        g[f\"{col}_lag_{l}\"] = g[col].shift(l)\n",
    "    for r in [3,7]:\n",
    "        g[f\"{col}_roll_mean_{r}\"] = g[col].shift(1).rolling(r).mean()\n",
    "        g[f\"{col}_roll_std_{r}\"]  = g[col].shift(1).rolling(r).std()\n",
    "    return g\n",
    "\n",
    "# ===============================\n",
    "# TRAIN\n",
    "# ===============================\n",
    "models = {}\n",
    "ranges = {}\n",
    "\n",
    "for loc, g in df.groupby(\"lokasi_clean\"):\n",
    "\n",
    "    models[loc] = {}\n",
    "    ranges[loc] = {}\n",
    "\n",
    "    for pol in POLLUTANTS:\n",
    "\n",
    "        g_feat = build_lag(g, pol).dropna()\n",
    "\n",
    "        # ðŸ”¥ FIX FEATURE SELECTION\n",
    "        FEATS = [c for c in g_feat.columns if (pol in c and (\"lag\" in c or \"roll\" in c))]\n",
    "        FEATS += TIME_FEATS + WEATHER\n",
    "\n",
    "        X = g_feat[FEATS]\n",
    "        y = g_feat[pol]\n",
    "\n",
    "        model = lgb.train(\n",
    "            {\n",
    "                \"objective\":\"regression\",\n",
    "                \"learning_rate\":0.03,\n",
    "                \"num_leaves\":63,\n",
    "                \"min_data_in_leaf\":20,\n",
    "                \"feature_fraction\":0.8,\n",
    "                \"bagging_fraction\":0.8,\n",
    "                \"bagging_freq\":1,\n",
    "                \"verbosity\":-1,\n",
    "                \"seed\":SEED,\n",
    "            },\n",
    "            lgb.Dataset(X,label=y),\n",
    "            num_boost_round=500,\n",
    "        )\n",
    "\n",
    "        models[loc][pol] = (model, FEATS)\n",
    "\n",
    "        # simpan range biar recursive stabil\n",
    "        ranges[loc][pol] = (y.min(), y.max())\n",
    "\n",
    "# ===============================\n",
    "# LOAD SUBMISSION\n",
    "# ===============================\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub[\"tanggal\"] = pd.to_datetime(sub[\"id\"].str.split(\"_\").str[0])\n",
    "sub[\"lokasi_clean\"] = sub[\"id\"].str.split(\"_\").str[1]\n",
    "sub = sub.sort_values([\"lokasi_clean\",\"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "# ===============================\n",
    "# FORECAST\n",
    "# ===============================\n",
    "results = []\n",
    "\n",
    "for loc, gsub in sub.groupby(\"lokasi_clean\"):\n",
    "\n",
    "    hist = df[df[\"lokasi_clean\"]==loc].sort_values(\"tanggal\").iloc[-30:]\n",
    "    hist_dict = {pol: hist[pol].tolist() for pol in POLLUTANTS}\n",
    "\n",
    "    for _, row in gsub.iterrows():\n",
    "        tgl = row[\"tanggal\"]\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        for pol in POLLUTANTS:\n",
    "            model, FEATS = models[loc][pol]\n",
    "\n",
    "            feat = {\n",
    "                \"month_sin\": np.sin(2*np.pi*tgl.month/12),\n",
    "                \"month_cos\": np.cos(2*np.pi*tgl.month/12),\n",
    "                \"doy_sin\": np.sin(2*np.pi*tgl.dayofyear/365),\n",
    "                \"doy_cos\": np.cos(2*np.pi*tgl.dayofyear/365),\n",
    "            }\n",
    "\n",
    "            # weather pakai last known value\n",
    "            for w in WEATHER:\n",
    "                feat[w] = hist[w].iloc[-1]\n",
    "\n",
    "            for l in [1,2,3,7]:\n",
    "                feat[f\"{pol}_lag_{l}\"] = hist_dict[pol][-l]\n",
    "\n",
    "            for r in [3,7]:\n",
    "                vals = hist_dict[pol][-r:]\n",
    "                feat[f\"{pol}_roll_mean_{r}\"] = np.mean(vals)\n",
    "                feat[f\"{pol}_roll_std_{r}\"]  = np.std(vals)\n",
    "\n",
    "            Xi = pd.DataFrame([feat])[FEATS]\n",
    "            pred = model.predict(Xi)[0]\n",
    "\n",
    "            # ðŸ”¥ STABILISASI RANGE\n",
    "            lo, hi = ranges[loc][pol]\n",
    "            pred = np.clip(pred, lo, hi)\n",
    "\n",
    "            hist_dict[pol].append(pred)\n",
    "            hist_dict[pol].pop(0)\n",
    "\n",
    "            preds.append(pred)\n",
    "\n",
    "        # ===============================\n",
    "        # MAX ISPU\n",
    "        # ===============================\n",
    "        max_pred = max(preds)\n",
    "\n",
    "        # ===============================\n",
    "        # MAP KATEGORI RESMI\n",
    "        # ===============================\n",
    "        if max_pred <= 50:\n",
    "            kategori = \"BAIK\"\n",
    "        elif max_pred <= 100:\n",
    "            kategori = \"SEDANG\"\n",
    "        else:\n",
    "            kategori = \"TIDAK SEHAT\"\n",
    "\n",
    "        results.append(kategori)\n",
    "\n",
    "# ===============================\n",
    "# SAVE\n",
    "# ===============================\n",
    "sub[\"kategori\"] = results\n",
    "sub[[\"id\",\"kategori\"]].to_csv(\"submission_F1.csv\", index=False)\n",
    "\n",
    "print(\"âœ… FINAL FIXED SUBMISSION GENERATED â†’ submission_FIXED.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ff870a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10800\\934582758.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_model = df_model.groupby(\"lokasi_clean\", group_keys=False).apply(make_target_lag)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10800\\934582758.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby(\"lokasi_clean\", group_keys=False).apply(make_lag_features)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10800\\934582758.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  valid_df = valid_df.groupby(\"lokasi_clean\", group_keys=False).apply(make_lag_features)\n"
     ]
    }
   ],
   "source": [
    "df_model = df.dropna(subset=[\"target\"]).reset_index(drop=True)\n",
    "\n",
    "def make_target_lag(g):\n",
    "    g = g.sort_values(\"tanggal\")\n",
    "    g[\"target_lag1\"] = g[\"target\"].shift(1)\n",
    "    g[\"target_lag2\"] = g[\"target\"].shift(2)\n",
    "    g[\"target_roll7\"] = g[\"target\"].shift(1).rolling(7).mean()\n",
    "    return g\n",
    "\n",
    "df_model = df_model.groupby(\"lokasi_clean\", group_keys=False).apply(make_target_lag)\n",
    "\n",
    "\n",
    "SPLIT_DATE = \"2024-01-01\"\n",
    "\n",
    "train_df = df_model[df_model[\"tanggal\"] < SPLIT_DATE].copy()\n",
    "valid_df = df_model[df_model[\"tanggal\"] >= SPLIT_DATE].copy()\n",
    "\n",
    "def make_lag_features(g):\n",
    "    g = g.sort_values(\"tanggal\")\n",
    "    for col in POLLUTANT_COLS:\n",
    "        g[f\"{col}_lag1\"] = g[col].shift(1)\n",
    "        g[f\"{col}_lag2\"] = g[col].shift(2)\n",
    "        g[f\"{col}_lag3\"] = g[col].shift(3)\n",
    "    return g\n",
    "\n",
    "train_df = train_df.groupby(\"lokasi_clean\", group_keys=False).apply(make_lag_features)\n",
    "valid_df = valid_df.groupby(\"lokasi_clean\", group_keys=False).apply(make_lag_features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4abfdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_loc = LabelEncoder()\n",
    "\n",
    "train_df[\"lokasi_enc\"] = le_loc.fit_transform(train_df[\"lokasi_clean\"])\n",
    "valid_df[\"lokasi_enc\"] = le_loc.transform(valid_df[\"lokasi_clean\"])\n",
    "\n",
    "# >>> TAMBAHAN WAJIB\n",
    "df_model[\"lokasi_enc\"] = le_loc.transform(df_model[\"lokasi_clean\"])\n",
    "\n",
    "FEATURES_F = (\n",
    "    [f\"{c}_lag1\" for c in POLLUTANT_COLS] +\n",
    "    [f\"{c}_lag2\" for c in POLLUTANT_COLS] +\n",
    "    [f\"{c}_lag3\" for c in POLLUTANT_COLS] +\n",
    "    [\"month\", \"dayofyear\", \"dayofweek\", \"lokasi_enc\"]\n",
    ")\n",
    "\n",
    "pollutant_models = {}\n",
    "\n",
    "for col in POLLUTANT_COLS:\n",
    "    X = train_df[FEATURES_F]\n",
    "    y = train_df[col]\n",
    "\n",
    "    dtrain = lgb.Dataset(X, label=y)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"l2\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"num_leaves\": 64,\n",
    "            \"verbosity\": -1,\n",
    "            \"seed\": 42,\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=300\n",
    "    )\n",
    "\n",
    "    pollutant_models[col] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c67b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPORAL_COLS = [\n",
    "    \"month_sin\",\"month_cos\",\n",
    "    \"doy_sin\",\"doy_cos\",\n",
    "    \"dow_sin\",\"dow_cos\",\n",
    "]\n",
    "\n",
    "ISPU_FEATURES = (\n",
    "    POLLUTANT_COLS +\n",
    "    WEATHER_COLS +\n",
    "    TEMPORAL_COLS +\n",
    "    [\"target_lag1\", \"target_lag2\", \"target_roll7\"] +\n",
    "    [\"lokasi_enc\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Macro-F1: 0.5298\n",
      "âœ… submission_direct.csv siap upload\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# DIRECT MULTI-HORIZON ISPU â€” FINAL CLEAN VERSION\n",
    "# =====================================================\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "HORIZON = 7\n",
    "FEATURES_DIRECT = ISPU_FEATURES + [\"horizon\"]\n",
    "\n",
    "# -------------------------------\n",
    "# BUILD DATASET DIRECT\n",
    "# -------------------------------\n",
    "def build_direct(df, horizon):\n",
    "    rows = []\n",
    "\n",
    "    for loc, g in df.groupby(\"lokasi_clean\"):\n",
    "        g = g.sort_values(\"tanggal\").reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(g) - horizon):\n",
    "            base = g.iloc[i]\n",
    "\n",
    "            for h in range(1, horizon + 1):\n",
    "                target = g.iloc[i + h][\"target\"]\n",
    "\n",
    "                feat = base[ISPU_FEATURES].to_dict()\n",
    "                feat[\"horizon\"] = h\n",
    "                feat[\"target\"] = target\n",
    "\n",
    "                rows.append(feat)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "train_direct = build_direct(train_df, HORIZON)\n",
    "valid_direct = build_direct(valid_df, HORIZON)\n",
    "\n",
    "X_train = train_direct[FEATURES_DIRECT]\n",
    "y_train = train_direct[\"target\"]\n",
    "\n",
    "X_valid = valid_direct[FEATURES_DIRECT]\n",
    "y_valid = valid_direct[\"target\"]\n",
    "\n",
    "# -------------------------------\n",
    "# CLASS WEIGHT\n",
    "# -------------------------------\n",
    "classes = np.array([0, 1, 2])\n",
    "cw = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "cw_dict = dict(zip(classes, cw))\n",
    "sample_weights = y_train.map(cw_dict).values\n",
    "\n",
    "# -------------------------------\n",
    "# TRAIN MODEL\n",
    "# -------------------------------\n",
    "dtrain = lgb.Dataset(X_train, label=y_train, weight=sample_weights)\n",
    "dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 3,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "direct_model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=700,\n",
    "    valid_sets=[dvalid],\n",
    "    callbacks=[lgb.log_evaluation(0)]  # silent\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# VALIDASI\n",
    "# -------------------------------\n",
    "probs = direct_model.predict(X_valid)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "\n",
    "print(\"Direct Macro-F1:\", round(f1_score(y_valid, preds, average=\"macro\"), 4))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# INFERENCE SUBMISSION (FINAL FIXED)\n",
    "# =====================================================\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "sub[\"tanggal\"] = pd.to_datetime(sub[\"id\"].str.split(\"_\").str[0])\n",
    "sub[\"lokasi_clean\"] = sub[\"id\"].str.split(\"_\").str[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "for loc in sub[\"lokasi_clean\"].unique():\n",
    "\n",
    "    # ===== history polutan =====\n",
    "    hist_pol = (\n",
    "        df_model[df_model[\"lokasi_clean\"] == loc]\n",
    "        .sort_values(\"tanggal\")\n",
    "        .iloc[-3:][POLLUTANT_COLS]\n",
    "        .values.tolist()\n",
    "    )\n",
    "\n",
    "    # ===== last feature row =====\n",
    "    last_row = (\n",
    "        df_model[df_model[\"lokasi_clean\"] == loc]\n",
    "        .sort_values(\"tanggal\")\n",
    "        .iloc[-1]\n",
    "    )\n",
    "\n",
    "    future = sub[sub[\"lokasi_clean\"] == loc].sort_values(\"tanggal\")\n",
    "\n",
    "    for h, (_, row) in enumerate(future.iterrows(), start=1):\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. FORECAST POLLUTAN\n",
    "        # -------------------------\n",
    "        feat_f = {\n",
    "            \"month\": row[\"tanggal\"].month,\n",
    "            \"dayofyear\": row[\"tanggal\"].dayofyear,\n",
    "            \"dayofweek\": row[\"tanggal\"].dayofweek,\n",
    "            \"lokasi_enc\": le_loc.transform([loc])[0],\n",
    "        }\n",
    "\n",
    "        for i, col in enumerate(POLLUTANT_COLS):\n",
    "            feat_f[f\"{col}_lag1\"] = hist_pol[-1][i]\n",
    "            feat_f[f\"{col}_lag2\"] = hist_pol[-2][i]\n",
    "            feat_f[f\"{col}_lag3\"] = hist_pol[-3][i]\n",
    "\n",
    "        Xf = pd.DataFrame([feat_f])\n",
    "\n",
    "        new_pol = []\n",
    "        for col in POLLUTANT_COLS:\n",
    "            p = pollutant_models[col].predict(Xf)[0]\n",
    "            p = np.clip(p, 0, train_df[col].quantile(0.995))\n",
    "            new_pol.append(p)\n",
    "\n",
    "        hist_pol.append(new_pol)\n",
    "        hist_pol.pop(0)\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. BUILD DIRECT FEATURES\n",
    "        # -------------------------\n",
    "        feat = last_row[ISPU_FEATURES].to_dict()\n",
    "\n",
    "        tgl = row[\"tanggal\"]\n",
    "\n",
    "        feat[\"month_sin\"] = np.sin(2*np.pi*tgl.month/12)\n",
    "        feat[\"month_cos\"] = np.cos(2*np.pi*tgl.month/12)\n",
    "\n",
    "        feat[\"doy_sin\"] = np.sin(2*np.pi*tgl.dayofyear/365)\n",
    "        feat[\"doy_cos\"] = np.cos(2*np.pi*tgl.dayofyear/365)\n",
    "\n",
    "        feat[\"dow_sin\"] = np.sin(2*np.pi*tgl.dayofweek/7)\n",
    "        feat[\"dow_cos\"] = np.cos(2*np.pi*tgl.dayofweek/7)\n",
    "\n",
    "\n",
    "        for i, col in enumerate(POLLUTANT_COLS):\n",
    "            feat[col] = new_pol[i]\n",
    "\n",
    "        feat[\"horizon\"] = min(h, HORIZON)\n",
    "\n",
    "        Xi = pd.DataFrame([feat])[FEATURES_DIRECT]\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. PREDICT ISPU\n",
    "        # -------------------------\n",
    "        prob = direct_model.predict(Xi)[0]\n",
    "        pred = int(np.argmax(prob))\n",
    "\n",
    "        results.append(pred)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE SUBMISSION\n",
    "# -------------------------------\n",
    "INV_LABEL_MAP = {0: \"BAIK\", 1: \"SEDANG\", 2: \"TIDAK SEHAT\"}\n",
    "\n",
    "sub[\"kategori\"] = [INV_LABEL_MAP[p] for p in results]\n",
    "sub[[\"id\", \"kategori\"]].to_csv(\"submission_direct.csv\", index=False)\n",
    "\n",
    "print(\"âœ… submission_direct.csv siap upload\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
